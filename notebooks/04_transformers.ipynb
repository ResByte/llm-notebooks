{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhM4u9xwqyic52Q9GiFNmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ResByte/llm-notebooks/blob/main/notebooks/04_transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Transformers from Scratch\n",
        "\n",
        "This notebook is replication of blogpost by Mat Miller(https://blog.matdmiller.com/posts/2023-06-10_transformers/notebook.html)\n",
        "\n",
        "He used it to explain the youtube video \"Lets build GPT\" by Andrej"
      ],
      "metadata": {
        "id": "XY3KcrbBPK_1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Getting Started"
      ],
      "metadata": {
        "id": "kDdL1GZuPiYb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKRn7hOPO_Vy",
        "outputId": "9ffb8931-17bf-45fe-819a-d5258d2bfaa4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-01-14 15:47:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.1’\n",
            "\n",
            "\rinput.txt.1           0%[                    ]       0  --.-KB/s               \rinput.txt.1         100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-01-14 15:47:35 (19.4 MB/s) - ‘input.txt.1’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# download the dataset\n",
        "# in this case, mini-shakespeare\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "print(f\"Length: {len(text)}\")\n",
        "print(f\"Initial sample: {text[:500]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yK-Hfl2Pryj",
        "outputId": "f753c617-cb06-4a20-c815-398663912264"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length: 1115394\n",
            "Initial sample: First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Tokenization\n",
        "\n",
        "In LLMs generally tokens are created at sub-word level, but in this case we are doing at char level"
      ],
      "metadata": {
        "id": "nEIWlyeaP_-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(list(set(text)))\n",
        "vocab_size = len(vocab)\n",
        "print(f\"size: {vocab_size}\")\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBI-aNaCP9Ey",
        "outputId": "a686380b-2b8d-47fa-d3d7-f0626a1991c7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size: 65\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to feed these tokens into model, these characters needs to be converted to numbers. In LLMs, these are called embeddings and generally a learned vector representation is used."
      ],
      "metadata": {
        "id": "bqZFP-f0QfbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "char2idx = {char:idx for idx, char in enumerate(vocab)}\n",
        "char2idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zx7H7etQbNR",
        "outputId": "d470c932-3af5-4bda-b75d-4dc2100a501c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '$': 3,\n",
              " '&': 4,\n",
              " \"'\": 5,\n",
              " ',': 6,\n",
              " '-': 7,\n",
              " '.': 8,\n",
              " '3': 9,\n",
              " ':': 10,\n",
              " ';': 11,\n",
              " '?': 12,\n",
              " 'A': 13,\n",
              " 'B': 14,\n",
              " 'C': 15,\n",
              " 'D': 16,\n",
              " 'E': 17,\n",
              " 'F': 18,\n",
              " 'G': 19,\n",
              " 'H': 20,\n",
              " 'I': 21,\n",
              " 'J': 22,\n",
              " 'K': 23,\n",
              " 'L': 24,\n",
              " 'M': 25,\n",
              " 'N': 26,\n",
              " 'O': 27,\n",
              " 'P': 28,\n",
              " 'Q': 29,\n",
              " 'R': 30,\n",
              " 'S': 31,\n",
              " 'T': 32,\n",
              " 'U': 33,\n",
              " 'V': 34,\n",
              " 'W': 35,\n",
              " 'X': 36,\n",
              " 'Y': 37,\n",
              " 'Z': 38,\n",
              " 'a': 39,\n",
              " 'b': 40,\n",
              " 'c': 41,\n",
              " 'd': 42,\n",
              " 'e': 43,\n",
              " 'f': 44,\n",
              " 'g': 45,\n",
              " 'h': 46,\n",
              " 'i': 47,\n",
              " 'j': 48,\n",
              " 'k': 49,\n",
              " 'l': 50,\n",
              " 'm': 51,\n",
              " 'n': 52,\n",
              " 'o': 53,\n",
              " 'p': 54,\n",
              " 'q': 55,\n",
              " 'r': 56,\n",
              " 's': 57,\n",
              " 't': 58,\n",
              " 'u': 59,\n",
              " 'v': 60,\n",
              " 'w': 61,\n",
              " 'x': 62,\n",
              " 'y': 63,\n",
              " 'z': 64}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2char = {idx:char for char, idx in char2idx.items()}\n",
        "idx2char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeBl71VnQ1AQ",
        "outputId": "92240c3b-6acc-4c39-dca9-ed7a9430be0b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '$',\n",
              " 4: '&',\n",
              " 5: \"'\",\n",
              " 6: ',',\n",
              " 7: '-',\n",
              " 8: '.',\n",
              " 9: '3',\n",
              " 10: ':',\n",
              " 11: ';',\n",
              " 12: '?',\n",
              " 13: 'A',\n",
              " 14: 'B',\n",
              " 15: 'C',\n",
              " 16: 'D',\n",
              " 17: 'E',\n",
              " 18: 'F',\n",
              " 19: 'G',\n",
              " 20: 'H',\n",
              " 21: 'I',\n",
              " 22: 'J',\n",
              " 23: 'K',\n",
              " 24: 'L',\n",
              " 25: 'M',\n",
              " 26: 'N',\n",
              " 27: 'O',\n",
              " 28: 'P',\n",
              " 29: 'Q',\n",
              " 30: 'R',\n",
              " 31: 'S',\n",
              " 32: 'T',\n",
              " 33: 'U',\n",
              " 34: 'V',\n",
              " 35: 'W',\n",
              " 36: 'X',\n",
              " 37: 'Y',\n",
              " 38: 'Z',\n",
              " 39: 'a',\n",
              " 40: 'b',\n",
              " 41: 'c',\n",
              " 42: 'd',\n",
              " 43: 'e',\n",
              " 44: 'f',\n",
              " 45: 'g',\n",
              " 46: 'h',\n",
              " 47: 'i',\n",
              " 48: 'j',\n",
              " 49: 'k',\n",
              " 50: 'l',\n",
              " 51: 'm',\n",
              " 52: 'n',\n",
              " 53: 'o',\n",
              " 54: 'p',\n",
              " 55: 'q',\n",
              " 56: 'r',\n",
              " 57: 's',\n",
              " 58: 't',\n",
              " 59: 'u',\n",
              " 60: 'v',\n",
              " 61: 'w',\n",
              " 62: 'x',\n",
              " 63: 'y',\n",
              " 64: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda x : [char2idx[char] for char in x]\n",
        "decode = lambda idxs: ''.join([idx2char[idx] for idx in idxs])\n",
        "print(\"Tokenize Hello world! :\", encode(\"Hello world!\"))\n",
        "print(\"Create string from tokens :\", decode([20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUrarCOjQ8Hs",
        "outputId": "e5682964-91dc-4f36-dcf1-8a1af9231508"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize Hello world! : [20, 43, 50, 50, 53, 1, 61, 53, 56, 50, 42, 2]\n",
            "Create string from tokens : Hello world!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create tensor embeddings for the dataset"
      ],
      "metadata": {
        "id": "dncHOe_IRauu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "JSm5S8jmYcvK"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text = torch.tensor(encode(text))\n",
        "encoded_text.shape, encoded_text.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhvJvS6mYd4m",
        "outputId": "9e1c17ad-c2d0-490f-e482-7d277e31361c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1115394]), torch.int64)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idiialQHYmJI",
        "outputId": "f53da3c0-e925-491c-b4e8-c3c12c9bbbca"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56,  ..., 45,  8,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_split_pct = 0.9\n",
        "train_split_idx = int(len(encoded_text)*train_split_pct)\n",
        "train_split_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5DJj4afYp3W",
        "outputId": "267f49a3-f8a5-42fb-8b89-deddb45a6a35"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = encoded_text[:train_split_idx]\n",
        "valid_data = encoded_text[train_split_idx:]\n",
        "print(f\"Train data: {len(train_data)}, Valid data: {len(valid_data)}, Train pct: {len(train_data)/len(encoded_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHTbM3GmxfgD",
        "outputId": "1b233638-3d84-4e19-cac6-42e4ec4952d5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data: 1003854, Valid data: 111540, Train pct: 0.8999994620734916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Context length is the minimum length of the seq used when training the transformer. Also referred to as block size. The transformer will be trained on each combination of tokens up to maximum context"
      ],
      "metadata": {
        "id": "1E0APHabyJ99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = 8\n",
        "for i in range(context_length):\n",
        "    x,y  = train_data[:i+1], train_data[i+1]\n",
        "    print(f\"idx: {i}, x: {x}, y:{y}, | decoded x: {decode(x.tolist())}, y: {decode(y[None].tolist())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGwEWV1ux4DT",
        "outputId": "5b4e581c-0240-4d7f-b0fa-ad2e632a6d75"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx: 0, x: tensor([18]), y:47, | decoded x: F, y: i\n",
            "idx: 1, x: tensor([18, 47]), y:56, | decoded x: Fi, y: r\n",
            "idx: 2, x: tensor([18, 47, 56]), y:57, | decoded x: Fir, y: s\n",
            "idx: 3, x: tensor([18, 47, 56, 57]), y:58, | decoded x: Firs, y: t\n",
            "idx: 4, x: tensor([18, 47, 56, 57, 58]), y:1, | decoded x: First, y:  \n",
            "idx: 5, x: tensor([18, 47, 56, 57, 58,  1]), y:15, | decoded x: First , y: C\n",
            "idx: 6, x: tensor([18, 47, 56, 57, 58,  1, 15]), y:47, | decoded x: First C, y: i\n",
            "idx: 7, x: tensor([18, 47, 56, 57, 58,  1, 15, 47]), y:58, | decoded x: First Ci, y: t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters"
      ],
      "metadata": {
        "id": "pBZj72in0o6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TORCH_SEED = 42\n",
        "torch.manual_seed(TORCH_SEED)\n",
        "context_length = 8\n",
        "batch_size = 4"
      ],
      "metadata": {
        "id": "_yYF25Fz0gRv"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Loader"
      ],
      "metadata": {
        "id": "EH45rZp41df-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(train_valid):\n",
        "    data = train_data if train_valid == 'train' else valid_data\n",
        "    data_len = len(data)\n",
        "    start_idxs = torch.randint(\n",
        "        high=len(data) - context_length,\n",
        "        size=(batch_size,))\n",
        "    x = torch.stack([data[i: i+context_length] for i in start_idxs])\n",
        "    y = torch.stack([data[i+1:i+context_length + 1] for i in start_idxs])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(f\"shape: {xb.shape}, xb: {xb}\")\n",
        "print(f\"Targets: Shape:{yb.shape} yb: {yb}\")\n",
        "\n",
        "for batch_idx in range(batch_size):\n",
        "    for seq_idx in range(context_length):\n",
        "        print(batch_idx, seq_idx)\n",
        "        context = xb[batch_idx,:seq_idx + 1]\n",
        "        target = yb[batch_idx, seq_idx]\n",
        "        print(f\"Given input: {context.tolist()} target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvbslR_j01xQ",
        "outputId": "93b070a6-995b-484d-8f39-8b2ba05a2237"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "shape: torch.Size([4, 8]), xb: tensor([[57,  1, 46, 47, 57,  1, 50, 53],\n",
            "        [ 1, 58, 46, 43, 56, 43,  1, 41],\n",
            "        [17, 26, 15, 17, 10,  0, 32, 53],\n",
            "        [57, 58,  6,  1, 61, 47, 58, 46]])\n",
            "Targets: Shape:torch.Size([4, 8]) yb: tensor([[ 1, 46, 47, 57,  1, 50, 53, 60],\n",
            "        [58, 46, 43, 56, 43,  1, 41, 39],\n",
            "        [26, 15, 17, 10,  0, 32, 53,  1],\n",
            "        [58,  6,  1, 61, 47, 58, 46,  0]])\n",
            "0 0\n",
            "Given input: [57] target: 1\n",
            "0 1\n",
            "Given input: [57, 1] target: 46\n",
            "0 2\n",
            "Given input: [57, 1, 46] target: 47\n",
            "0 3\n",
            "Given input: [57, 1, 46, 47] target: 57\n",
            "0 4\n",
            "Given input: [57, 1, 46, 47, 57] target: 1\n",
            "0 5\n",
            "Given input: [57, 1, 46, 47, 57, 1] target: 50\n",
            "0 6\n",
            "Given input: [57, 1, 46, 47, 57, 1, 50] target: 53\n",
            "0 7\n",
            "Given input: [57, 1, 46, 47, 57, 1, 50, 53] target: 60\n",
            "1 0\n",
            "Given input: [1] target: 58\n",
            "1 1\n",
            "Given input: [1, 58] target: 46\n",
            "1 2\n",
            "Given input: [1, 58, 46] target: 43\n",
            "1 3\n",
            "Given input: [1, 58, 46, 43] target: 56\n",
            "1 4\n",
            "Given input: [1, 58, 46, 43, 56] target: 43\n",
            "1 5\n",
            "Given input: [1, 58, 46, 43, 56, 43] target: 1\n",
            "1 6\n",
            "Given input: [1, 58, 46, 43, 56, 43, 1] target: 41\n",
            "1 7\n",
            "Given input: [1, 58, 46, 43, 56, 43, 1, 41] target: 39\n",
            "2 0\n",
            "Given input: [17] target: 26\n",
            "2 1\n",
            "Given input: [17, 26] target: 15\n",
            "2 2\n",
            "Given input: [17, 26, 15] target: 17\n",
            "2 3\n",
            "Given input: [17, 26, 15, 17] target: 10\n",
            "2 4\n",
            "Given input: [17, 26, 15, 17, 10] target: 0\n",
            "2 5\n",
            "Given input: [17, 26, 15, 17, 10, 0] target: 32\n",
            "2 6\n",
            "Given input: [17, 26, 15, 17, 10, 0, 32] target: 53\n",
            "2 7\n",
            "Given input: [17, 26, 15, 17, 10, 0, 32, 53] target: 1\n",
            "3 0\n",
            "Given input: [57] target: 58\n",
            "3 1\n",
            "Given input: [57, 58] target: 6\n",
            "3 2\n",
            "Given input: [57, 58, 6] target: 1\n",
            "3 3\n",
            "Given input: [57, 58, 6, 1] target: 61\n",
            "3 4\n",
            "Given input: [57, 58, 6, 1, 61] target: 47\n",
            "3 5\n",
            "Given input: [57, 58, 6, 1, 61, 47] target: 58\n",
            "3 6\n",
            "Given input: [57, 58, 6, 1, 61, 47, 58] target: 46\n",
            "3 7\n",
            "Given input: [57, 58, 6, 1, 61, 47, 58, 46] target: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Bigram Model\n",
        "\n",
        "Model predicts probability of one token following another."
      ],
      "metadata": {
        "id": "4KNZGI_XGxxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "RiIvRAagGxen"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(TORCH_SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwMizf454hOZ",
        "outputId": "49e1c815-8a0c-43bb-a08c-23e3cc84179a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7aba6c296830>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        self.token_embedding_table = nn.Embedding(\n",
        "            num_embeddings=self.vocab_size,\n",
        "            embedding_dim=self.vocab_size\n",
        "            )\n",
        "    def forward(self, idx, targets=None):\n",
        "        # both idx and targets are Batch, Time array of int\n",
        "        logits = self.token_embedding_table(idx)\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits_reshaped = logits.view(B*T, C)\n",
        "            targets_reshaped = targets.view(B*T)\n",
        "            loss = F.cross_entropy(\n",
        "                input=logits_reshaped,\n",
        "                target=targets_reshaped\n",
        "            )\n",
        "        else:\n",
        "            loss=None\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is B, T array of indicies in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get preds\n",
        "            logits, loss = self(idx)\n",
        "\n",
        "            # get the last time step from logits\n",
        "            logits_last_timestep = logits[:, -1, :]\n",
        "            print(f\"Shape of logits time stamp: {logits_last_timestep.shape}\")\n",
        "\n",
        "            # apply softmax\n",
        "            probs = F.softmax(input=logits_last_timestep, dim=-1)\n",
        "\n",
        "            # sample from probs distribution\n",
        "            idx_next = torch.multinomial(\n",
        "                input=probs,\n",
        "                num_samples=1\n",
        "            )\n",
        "\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "YgJlfTfPLQvE"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model = BigramLanguageModel(vocab_size=vocab_size)"
      ],
      "metadata": {
        "id": "bmMjSYcDqAZL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logits, loss = bigram_model(xb, yb)"
      ],
      "metadata": {
        "id": "SH2KCE6xqdgq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Loss: {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhPm8u41qfpv",
        "outputId": "069ae100-c14c-4703-bc82-dd7cf51a8439"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 4.731735706329346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1), dtype=torch.long)"
      ],
      "metadata": {
        "id": "2s9M52c3q1RJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"100 Generated Tokens:\",\n",
        "      decode(bigram_model.generate(idx, max_new_tokens=100)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5ItbWsrC2c",
        "outputId": "f6944008-237d-47ac-eda0-0159bc833017"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "Shape of logits time stamp: torch.Size([1, 65])\n",
            "100 Generated Tokens: \n",
            "tLtZ&A.b-nw.?WmzgOnL$ax.Wm-Uq!\n",
            "m-G!f.?TUMtryaLJoaajoYDZf: KivGqQIsDAnH.aj-G?':k-G?Nd!bikrta.?dQXhqFY\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to train the model\n",
        "optimizer = torch.optim.Adam(\n",
        "    params=bigram_model.parameters(),\n",
        "    lr=1e-3\n",
        ")"
      ],
      "metadata": {
        "id": "ZhQU4C8mthn5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v5SBIQjV18ID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}