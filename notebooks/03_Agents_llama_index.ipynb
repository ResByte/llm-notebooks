{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ResByte/llm-notebooks/blob/main/notebooks/03_Agents_llama_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bb8ce37-376c-4af3-b6c1-f75dfe930f23",
      "metadata": {
        "id": "8bb8ce37-376c-4af3-b6c1-f75dfe930f23"
      },
      "source": [
        "# LLM Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d5ecd70-4815-4ac8-b72e-9862a09e558b",
      "metadata": {
        "id": "5d5ecd70-4815-4ac8-b72e-9862a09e558b"
      },
      "source": [
        "Using llama-cpp-python load an existing mistral-7b model. This model is downloaded from HF and is in gguf format(updated format from ggml)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7b973291-0f7a-4671-849e-8f1a8cf75eb4",
      "metadata": {
        "id": "7b973291-0f7a-4671-849e-8f1a8cf75eb4",
        "outputId": "0e2369b3-c263-4906-dcd7-903059f25e21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -q llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q llama-index"
      ],
      "metadata": {
        "id": "TIRBSdxcHQ6F",
        "outputId": "347771e4-99b0-46e8-8133-eb4ab33b2c38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TIRBSdxcHQ6F",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is present in the HF hub."
      ],
      "metadata": {
        "id": "9kk4eG0YIcNy"
      },
      "id": "9kk4eG0YIcNy"
    },
    {
      "cell_type": "code",
      "source": [
        "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf?download=true\""
      ],
      "metadata": {
        "id": "VcvKJr6KIXI6"
      },
      "id": "VcvKJr6KIXI6",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "08a4fc13-20b3-4f81-820e-53499c19308e",
      "metadata": {
        "id": "08a4fc13-20b3-4f81-820e-53499c19308e"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms import LlamaCPP\n",
        "from llama_index.llms.llama_utils import (\n",
        "    messages_to_prompt,\n",
        "    completion_to_prompt,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ed05443d-fc20-4be0-bebc-de232773c9ed",
      "metadata": {
        "scrolled": true,
        "id": "ed05443d-fc20-4be0-bebc-de232773c9ed",
        "outputId": "ebe0148b-526f-4064-d3f2-6a6e769cfd11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading url https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf?download=true to path /tmp/llama_index/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf?download=true\n",
            "total size (MB): 4368.44\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4167it [00:30, 135.98it/s]                          \n",
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ],
      "source": [
        "llm = LlamaCPP(\n",
        "    model_url=model_url,\n",
        "    model_path=None,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=256,\n",
        "    context_window=3900,\n",
        "    generate_kwargs={},\n",
        "    model_kwargs={'n_gpu_layers':1},\n",
        "    messages_to_prompt=messages_to_prompt,\n",
        "    completion_to_prompt=completion_to_prompt,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "3ef77a60-6f9c-41a3-8a7e-03d3428ff054",
      "metadata": {
        "id": "3ef77a60-6f9c-41a3-8a7e-03d3428ff054"
      },
      "outputs": [],
      "source": [
        "response = llm.complete(\"Hello! Can you tell me a poem about cats and dogs?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "df1f8c6b-fc5d-416d-a984-65b0b14be27f",
      "metadata": {
        "id": "df1f8c6b-fc5d-416d-a984-65b0b14be27f",
        "outputId": "40d6e7cd-da41-4044-9e65-929761b8f2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Sure, here's a short poem about cats and dogs:\n",
            "\n",
            "Cats and dogs, they both have fur,\n",
            "But cats are more independent, that's for sure.\n",
            "Dogs love to play and run around,\n",
            "While cats prefer to lounge and be found.\n",
            "\n",
            "Cats are known for their graceful leaps,\n",
            "And dogs are great at fetching sticks and balls.\n",
            "Both animals have unique personalities,\n",
            "And make wonderful companions for us all.\n",
            "\n",
            "So whether you prefer cats or dogs,\n",
            "Just remember to give them lots of love and hugs.\n"
          ]
        }
      ],
      "source": [
        "print(response.text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "nkUD3wZiJvKO"
      },
      "id": "nkUD3wZiJvKO",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define calculation tool for our agent\n",
        "def multiply(a: int, b: int)->int:\n",
        "    return a * b\n",
        "\n",
        "def add(a: int, b: int)->int:\n",
        "    return a + b"
      ],
      "metadata": {
        "id": "ViM9oQyJJ0MJ"
      },
      "id": "ViM9oQyJJ0MJ",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.tools import BaseTool, FunctionTool\n",
        "\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)"
      ],
      "metadata": {
        "id": "aBB4thlkKUTk"
      },
      "id": "aBB4thlkKUTk",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Sequence, List\n",
        "from llama_index.llms  import ChatMessage"
      ],
      "metadata": {
        "id": "CxzaUyFNKOFh"
      },
      "id": "CxzaUyFNKOFh",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomAgent:\n",
        "    def __init__(\n",
        "            self,\n",
        "            llm,\n",
        "            tools=[],\n",
        "            chat_history=[]\n",
        "            ):\n",
        "        self._llm = llm\n",
        "        self._tools = {tool.metadata.name: tool for tool in tools}\n",
        "        self._chat_history = chat_history\n",
        "\n",
        "    def reset(self):\n",
        "        self._chat_history = []\n",
        "\n",
        "    def chat(self, message:str):\n",
        "        chat_history = self._chat_history\n",
        "        chat_history.append(ChatMessage(role='user', content=message))\n",
        "\n",
        "        tools = [\n",
        "            tool.metadata.to_openai_tool() for _, tool in self._tools.items()\n",
        "            ]\n",
        "\n",
        "        generated_message = self._llm.chat(chat_history, tools=tools).message\n",
        "        additional_kwargs = generated_message.additional_kwargs\n",
        "        chat_history.append(generated_message)\n",
        "\n",
        "        tool_calls = generated_message.additional_kwargs.get(\"tool_calls\", None)\n",
        "        if tool_calls is not None:\n",
        "            for tool_call in tool_calls:\n",
        "                function_message = self._call_function(tool_call)\n",
        "                chat_history.append(function_message)\n",
        "                generated_message = self._llm.chat(chat_history).message\n",
        "                chat_history.append(generated_message)\n",
        "        return generated_message.content\n",
        "\n",
        "    def _call_function(self, tool_call):\n",
        "        id_ = tool_call['id']\n",
        "        function_call = tool_call['function']\n",
        "        tool = self._tools[function_call['name']]\n",
        "        output = tool(**json.loads(function_call['arguments']))\n",
        "        print(f\"Calling function: {function_call['name']}\")\n",
        "        return ChatMessage(\n",
        "            name=function_call['name'],\n",
        "            content = str(output),\n",
        "            role='tool',\n",
        "            additional_kwargs={\n",
        "                'tool_call_id': id_,\n",
        "                'name': function_call['name']\n",
        "            }\n",
        "        )\n"
      ],
      "metadata": {
        "id": "L1etlmXgKFHc"
      },
      "id": "L1etlmXgKFHc",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = CustomAgent(llm=llm, tools=[multiply_tool, add_tool])"
      ],
      "metadata": {
        "id": "o4ZS1vyVMr-c"
      },
      "id": "o4ZS1vyVMr-c",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat('Hi')"
      ],
      "metadata": {
        "id": "P4YFJFBUM4ZV",
        "outputId": "266737a2-d054-48fe-af28-2b2a0e75c016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "id": "P4YFJFBUM4ZV",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"What is 2123 * 215123\")"
      ],
      "metadata": {
        "id": "IQ_G3JMTNF4n",
        "outputId": "2486eba4-a4f3-436a-c179-03506e22f687",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "id": "IQ_G3JMTNF4n",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The result of multiplying 2123 by 215123 is 470689199.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"The answer is wrong\")"
      ],
      "metadata": {
        "id": "_h36ROncNrUd",
        "outputId": "19f7bd2f-93f6-4cf2-85ba-66d541f9668b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "id": "_h36ROncNrUd",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' I apologize for the mistake in my previous response. The correct answer to your question is:\\n\\nThe result of multiplying 2123 by 215123 is 46089749.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.chat(\"What is (121 * 3) + 42?\")"
      ],
      "metadata": {
        "id": "4CgE0TKbN583",
        "outputId": "17d349e8-2599-42c2-afa5-be36ed9425a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "id": "4CgE0TKbN583",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The expression (121 \\\\* 3) + 42 evaluates to 363 + 42, which equals 405.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-rwt7n0GOJg0"
      },
      "id": "-rwt7n0GOJg0",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.17"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}